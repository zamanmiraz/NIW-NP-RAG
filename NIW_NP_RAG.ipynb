{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1zJ4zntI7ru5RgHl6_osLo8feAIIhUmVD",
      "authorship_tag": "ABX9TyNGcaHeSlGgDlZgzq4afJHG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zamanmiraz/NIW-NP-RAG/blob/main/NIW_NP_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "StNS6luj1vPm"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/zamanmiraz/NIW-NP-RAG.git\n",
        "%cd NIW-NP-RAG\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchvision torchaudio\n",
        "! pip install --upgrade langchain langchain-community\n",
        "! pip install --upgrade langchain-experimental\n",
        "! pip install pypdf\n",
        "! pip install -qU langchain-huggingface\n",
        "! pip install faiss-cpu\n",
        "! pip install -qU \"langchain[google-genai]\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "5D8YUeBkQLw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "import pathlib\n",
        "from rich_theme_manager import Theme, ThemeManager\n",
        "\n",
        "THEMES = [\n",
        "    Theme(\n",
        "        name=\"dark\",\n",
        "        description=\"Dark mode theme\",\n",
        "        tags=[\"dark\"],\n",
        "        styles={\n",
        "            \"repr.own\": Style(color=\"#e87d3e\", bold=True),     # Class names\n",
        "            \"repr.tag_name\": \"cyan\",                           # Tag names\n",
        "            \"repr.call\": \"bright_magenta\",                     # Function calls\n",
        "            \"repr.str\": \"bright_green\",                        # Strings\n",
        "            \"repr.number\": \"bright_red\",                       # Numbers\n",
        "            \"repr.none\": \"dim white\",                          # None\n",
        "            \"repr.attrib_name\": Style(color=\"#e87d3e\", bold=True),\n",
        "            \"repr.attrib_value\": \"bright_blue\",\n",
        "            \"default\": \"bright_white on black\",                # No yellow background\n",
        "        },\n",
        "    ),\n",
        "    Theme(\n",
        "        name=\"light\",\n",
        "        description=\"Light mode theme\",\n",
        "        styles={\n",
        "            \"repr.own\": Style(color=\"#e87d3e\", bold=True),\n",
        "            \"repr.tag_name\": Style(color=\"#0077cc\", bold=True),\n",
        "            \"repr.call\": Style(color=\"#800080\", bold=True),    # Purple instead of yellow\n",
        "            \"repr.str\": Style(color=\"#008080\", bold=True),\n",
        "            \"repr.number\": Style(color=\"#ff4500\", bold=True),\n",
        "            \"repr.none\": Style(color=\"#808080\", bold=True),\n",
        "            \"repr.attrib_name\": Style(color=\"#e87d3e\", bold=True),\n",
        "            \"repr.attrib_value\": \"bright_blue\",\n",
        "            \"default\": Style(color=\"#000000\", bgcolor=\"#ffffff\"),  # White background, no yellow\n",
        "        },\n",
        "    ),\n",
        "]\n",
        "\n",
        "theme_dir = pathlib.Path(\"themes\").expanduser()\n",
        "theme_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "theme_manager = ThemeManager(theme_dir=theme_dir, themes=THEMES)\n",
        "theme_manager.list_themes()\n",
        "\n",
        "dark = theme_manager.get(\"dark\")\n",
        "light = theme_manager.get(\"light\")\n",
        "\n",
        "console = Console(theme=light)\n"
      ],
      "metadata": {
        "id": "gOfQNc4AD5Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "# from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import glob # Import the glob module\n",
        "import fitz # Import fitz for read_pdf_to_string\n",
        "from transformers import pipeline\n",
        "import torch # Import torch to check for CUDA availability"
      ],
      "metadata": {
        "id": "Eh0KaDeNRs9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "P4aaLJEVIRh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- PROMPTS ---\n",
        "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
        "<document>\n",
        "{doc_content}\n",
        "</document>\n",
        "\"\"\"\n",
        "\n",
        "CHUNK_CONTEXT_PROMPT = \"\"\"\n",
        "Here is the chunk we want to situate within the whole document\n",
        "<chunk>\n",
        "{chunk_content}\n",
        "</chunk>\n",
        "\n",
        "Please give a short succinct context to situate this chunk within the overall document\n",
        "for the purposes of improving search retrieval of the chunk.\n",
        "Answer only with the succinct context and nothing else.\n",
        "\"\"\"\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "pGliIscXi3vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file = \"/content/drive/MyDrive/Data/uscis_aao_pdfs/MAR052025_03B5203.pdf\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type='percentile', breakpoint_threshold_amount=90)\n",
        "loader = PyPDFLoader(pdf_file)\n",
        "documents = loader.load()\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Csre8Wh_0GCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_vector_store.save_local(\"/content/drive/MyDrive/Data/chunks_vector_store\")"
      ],
      "metadata": {
        "id": "uGithXee36YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- PROMPTS ---\n",
        "DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
        "<document>\n",
        "{doc_content}\n",
        "</document>\n",
        "\"\"\"\n",
        "\n",
        "CHUNK_CONTEXT_PROMPT = \"\"\"\n",
        "Here is the chunk we want to situate within the whole document\n",
        "<chunk>\n",
        "{chunk_content}\n",
        "</chunk>\n",
        "\n",
        "Please give a short succinct context to situate this chunk within the overall document\n",
        "for the purposes of improving search retrieval of the chunk.\n",
        "Answer only with the succinct context and nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# --- GLOBAL PROMPT TEMPLATE ---\n",
        "CHUNK_CONTEXT_PROMPT = (\n",
        "    \"Given the following text chunk:\\n\\n\"\n",
        "    \"{chunk_content}\\n\\n\"\n",
        "    \"Describe its broader context in one or two sentences.\"\n",
        ")\n",
        "\n",
        "\n",
        "# --- FUNCTION: situate_context ---\n",
        "\n",
        "# Load a lightweight summarization model\n",
        "# Explicitly set the device to 'cuda' if available, otherwise 'cpu'\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\",   # or \"google/pegasus-xsum\"\n",
        "    device=device # Use the determined device\n",
        ")\n",
        "\n",
        "def situate_context(doc: str, chunk: str) -> str:\n",
        "    \"\"\"\n",
        "    Safely generate a short context for a chunk using a local summarization model,\n",
        "    truncating long documents to avoid IndexError.\n",
        "    \"\"\"\n",
        "    # Limit document length (BART can handle up to ~1024 tokens ≈ 4000 chars)\n",
        "    truncated_doc = doc[:3500]  # safe buffer\n",
        "    truncated_chunk = chunk[:1500]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Here is the document:\n",
        "<document>\n",
        "{truncated_doc}\n",
        "</document>\n",
        "\n",
        "Focus on this chunk:\n",
        "<chunk>\n",
        "{truncated_chunk}\n",
        "</chunk>\n",
        "\n",
        "Briefly describe how this chunk fits into the document (1–2 sentences).\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        summary = summarizer(prompt, max_length=60, min_length=10, do_sample=False)\n",
        "        return summary[0][\"summary_text\"].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Summarization failed for chunk: {e}\")\n",
        "        return \"Context unavailable due to summarization limit.\"\n",
        "\n",
        "\n",
        "# def situate_context(doc: str, chunk: str) -> str:\n",
        "#     \"\"\"\n",
        "#     Use Gemini to describe the context of a given text chunk.\n",
        "#     \"\"\"\n",
        "#     model = \"models/gemini-2.0-flash-001\"  # Include version suffix\n",
        "\n",
        "#     response = client.models.generate_content(\n",
        "#         model=model,\n",
        "#         contents=[\n",
        "#             {\n",
        "#                 \"role\": \"user\",\n",
        "#                 \"parts\": [{\"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk)}],\n",
        "#             },\n",
        "#         ],\n",
        "#     )\n",
        "\n",
        "#     return response.text.strip()\n",
        "\n",
        "\n",
        "# --- FUNCTION: read_pdf_to_string ---\n",
        "def read_pdf_to_string(path):\n",
        "    \"\"\"\n",
        "    Read a PDF document and return its text content.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(path)\n",
        "    content = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        content += page.get_text()\n",
        "    doc.close()\n",
        "    return content\n",
        "\n",
        "\n",
        "# --- FUNCTION: replace_t_with_space ---\n",
        "def replace_t_with_space(list_of_documents):\n",
        "    \"\"\"\n",
        "    Replace tab characters with spaces in all document page contents.\n",
        "    \"\"\"\n",
        "    for doc in list_of_documents:\n",
        "        doc.page_content = doc.page_content.replace('\\t', ' ')\n",
        "    return list_of_documents\n",
        "\n",
        "\n",
        "# --- FUNCTION: encode_pdf ---\n",
        "def encode_pdf(directory_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes all PDF files into a FAISS vector store using Hugging Face embeddings.\n",
        "    \"\"\"\n",
        "    all_documents = []\n",
        "    pdf_files = glob.glob(f\"{directory_path}/*.pdf\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        loader = PyPDFLoader(pdf_file)\n",
        "        documents = loader.load()\n",
        "        all_documents.extend(documents)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(all_documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "# --- FUNCTION: encode_pdf_semantic ---\n",
        "def encode_pdf_semantic(directory_path):\n",
        "    \"\"\"\n",
        "    Encodes all PDF files using Semantic Chunking and Hugging Face embeddings.\n",
        "    Adds contextual summaries for each chunk using Gemini.\n",
        "    \"\"\"\n",
        "    all_documents = []\n",
        "    pdf_files = glob.glob(f\"{directory_path}/*.pdf\")\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    text_splitter = SemanticChunker(\n",
        "        embeddings,\n",
        "        breakpoint_threshold_type='percentile',\n",
        "        breakpoint_threshold_amount=90,\n",
        "    )\n",
        "    # using tqdm here\n",
        "    for pdf_file in pdf_files:\n",
        "        loader = PyPDFLoader(pdf_file)\n",
        "        documents = loader.load()\n",
        "        texts = text_splitter.split_documents(documents)\n",
        "        pdf_text = read_pdf_to_string(pdf_file)\n",
        "        # for chunk in texts:\n",
        "        #     chunk.metadata['context'] = situate_context(\n",
        "        #         doc=pdf_text,\n",
        "        #         chunk=chunk.page_content\n",
        "        #     )\n",
        "\n",
        "        all_documents.extend(texts)\n",
        "\n",
        "    cleaned_texts = replace_t_with_space(all_documents)\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "# --- FUNCTION: retrieve_context_per_question ---\n",
        "def retrieve_context_per_question(question, chunks_query_retriever):\n",
        "    \"\"\"\n",
        "    Retrieves relevant document chunks for a question.\n",
        "    \"\"\"\n",
        "    docs = chunks_query_retriever.invoke(question)\n",
        "    context = [doc.page_content for doc in docs]\n",
        "    urls = list({doc.metadata.get(\"source\") for doc in docs if doc.metadata.get(\"source\")})\n",
        "    return context, urls\n",
        "\n",
        "\n",
        "# --- FUNCTION: show_context ---\n",
        "def show_context(context):\n",
        "    \"\"\"\n",
        "    Print retrieved context chunks.\n",
        "    \"\"\"\n",
        "    for i, c in enumerate(context):\n",
        "        print(f\"Context {i + 1}:\")\n",
        "        print(c)\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "Y2BBhgzELoj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Data/uscis_aao_pdfs\"\n",
        "# load the pdf document\n",
        "chunks_vector_store = encode_pdf_semantic(path)"
      ],
      "metadata": {
        "id": "oLkUkw1j87vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "personal_statement = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "supporting_pos = \"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "resume = \"\"\"\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7FC-ger7AinX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# --- Retrieve Top Context Chunks ---\n",
        "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 20})\n",
        "\n",
        "# # --- Define Personal Statement ---\n",
        "# personal_statement = \"\"\"\n",
        "# I am the petitioner and beneficiary for this I-140 petition. I would like to take this opportunity\n",
        "# to explain how I intend to continue my work in the United States. My primary focus is on projects\n",
        "# with the Department of Energy (DoE) and initiatives integrating Web3 infrastructure for secure\n",
        "# energy data management. Through these efforts, I aim to improve the efficiency, transparency, and\n",
        "# sustainability of national energy systems.\n",
        "# \"\"\"\n",
        "\n",
        "# --- Define Query with Source Awareness ---\n",
        "test_query = (\n",
        "    \"Based on the following personal statement and other supporting document, identify specific areas that need improvement \"\n",
        "    \"to strengthen my National Interest Waiver (NIW) case. Provide feedback for each NIW prong \"\n",
        "    \"and connect your analysis to evidence or examples drawn from the retrieved context.\\n\\n\"\n",
        "    \"Be explicit in citing where each piece of information comes from — for example, mention the \"\n",
        "    \"document title, case type, or link if available.\\n\\n\"\n",
        "    \"Personal Statement:\\n\"\n",
        "    f\"{personal_statement}\\n\\n\"\n",
        "    \"When applicable, include the following format for evidence reference:\\n\"\n",
        "    \"- (Source: [Document Title or Link])\"\n",
        "    f\"{supporting_pos}\\n\\n\"\n",
        "    f\"{resume} \\n \\n\"\n",
        ")\n",
        "\n",
        "# --- Retrieve Context ---\n",
        "context = retrieve_context_per_question(test_query, chunks_query_retriever)\n",
        "show_context(context)\n",
        "\n",
        "# --- SYSTEM PROMPT FOR RAG (NIW) ---\n",
        "system_prompt = (\n",
        "    \"You are an expert immigration Q&A assistant specializing in National Interest Waiver (NIW) petitions. \"\n",
        "    \"Your role is to describe what occurred in the retrieved context, focusing only on the information provided. \"\n",
        "    \"You must not add external knowledge, interpretation, or personal evaluation.\\n\\n\"\n",
        "\n",
        "    \"Your task is to summarize what the context shows — such as outcomes, reasoning, or findings — \"\n",
        "    \"that relate to NIW petitions and their approval or denial patterns. \"\n",
        "    \"Do not make predictions, judgments, or offer advice. \"\n",
        "    \"Base your response strictly and exclusively on the retrieved context.\\n\\n\"\n",
        "\n",
        "    \"Always specify your sources clearly by mentioning the document title, case identifier, or link \"\n",
        "    \"each time you reference contextual evidence.\\n\\n\"\n",
        "\n",
        "    \"Structure your description according to the three NIW prongs, using only details that appear in the context:\\n\"\n",
        "    \"1️⃣ **Substantial Merit and National Importance** — Describe how the applicant’s field or work was treated \"\n",
        "    \"in the context (e.g., what was considered nationally important or lacking merit). Include examples of outcomes if mentioned.\\n\"\n",
        "    \"2️⃣ **Well-Positioned to Advance the Proposed Endeavor** — Describe what the context reveals about how petitioners \"\n",
        "    \"demonstrated their qualifications, achievements, or future plans, and how USCIS or AAO evaluated those aspects.\\n\"\n",
        "    \"3️⃣ **Beneficial to the United States** — Describe what the retrieved materials say about how granting the waiver \"\n",
        "    \"benefits the U.S., or what reasons were given when such benefit was found insufficient.\\n\\n\"\n",
        "\n",
        "    \"⚠️ Important: You are only describing and summarizing what the retrieved context states. \"\n",
        "    \"Do not analyze, speculate, or generate new conclusions beyond it. \"\n",
        "    \"If a detail is missing, explicitly say that the context does not contain that information.\\n\\n\"\n",
        "\n",
        "    \"--- Retrieved Context ---\\n{context}\"\n",
        ")\n",
        "\n",
        "\n",
        "# --- Compose Final Prompt ---\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# --- Generate Response ---\n",
        "result = llm.invoke(\n",
        "    prompt.format_prompt(context=context, input=test_query).to_messages()\n",
        ")\n",
        "console.print(result)\n"
      ],
      "metadata": {
        "id": "-R5J7lW4104V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}