{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1zJ4zntI7ru5RgHl6_osLo8feAIIhUmVD",
      "authorship_tag": "ABX9TyMUoglrqW2yUeZqmQaEz6mZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zamanmiraz/NIW-NP-RAG/blob/main/NIW_NP_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "StNS6luj1vPm"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/zamanmiraz/NIW-NP-RAG.git\n",
        "%cd NIW-NP-RAG\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --force-reinstall torch==2.3.1 torchvision torchaudio\n",
        "! pip install --force-reinstall transformers==4.41.2\n",
        "! pip install --upgrade langchain langchain-community\n",
        "! pip install --upgrade langchain-experimental\n",
        "! pip install pypdf\n",
        "! pip install -qU langchain-huggingface\n",
        "! pip install faiss-cpu\n",
        "! pip install -qU \"langchain[google-genai]\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "5D8YUeBkQLw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "import pathlib\n",
        "from rich_theme_manager import Theme, ThemeManager\n",
        "\n",
        "THEMES = [\n",
        "    Theme(\n",
        "        name=\"dark\",\n",
        "        description=\"Dark mode theme\",\n",
        "        tags=[\"dark\"],\n",
        "        styles={\n",
        "            \"repr.own\": Style(color=\"#e87d3e\", bold=True),      # Class names\n",
        "            \"repr.tag_name\": \"dim cyan\",                        # Adjust tag names\n",
        "            \"repr.call\": \"bright_yellow\",                       # Function calls and other symbols\n",
        "            \"repr.str\": \"bright_green\",                         # String representation\n",
        "            \"repr.number\": \"bright_red\",                        # Numbers\n",
        "            \"repr.none\": \"dim white\",                           # None\n",
        "            \"repr.attrib_name\": Style(color=\"#e87d3e\", bold=True),    # Attribute names\n",
        "            \"repr.attrib_value\": \"bright_blue\",                 # Attribute values\n",
        "            \"default\": \"bright_white on black\"                  # Default text and background\n",
        "        },\n",
        "    ),\n",
        "    Theme(\n",
        "        name=\"light\",\n",
        "        description=\"Light mode theme\",\n",
        "        styles={\n",
        "            \"repr.own\": Style(color=\"#22863a\", bold=True),          # Class names\n",
        "            \"repr.tag_name\": Style(color=\"#00bfff\", bold=True),     # Adjust tag names\n",
        "            \"repr.call\": Style(color=\"#ffff00\", bold=True),         # Function calls and other symbols\n",
        "            \"repr.str\": Style(color=\"#008080\", bold=True),          # String representation\n",
        "            \"repr.number\": Style(color=\"#ff6347\", bold=True),       # Numbers\n",
        "            \"repr.none\": Style(color=\"#808080\", bold=True),         # None\n",
        "            \"repr.attrib_name\": Style(color=\"#ffff00\", bold=True),  # Attribute names\n",
        "            \"repr.attrib_value\": Style(color=\"#008080\", bold=True), # Attribute values\n",
        "            \"default\": Style(color=\"#000000\", bgcolor=\"#ffffff\"),   # Default text and background\n",
        "        },\n",
        "    ),\n",
        "]\n",
        "\n",
        "theme_dir = pathlib.Path(\"themes\").expanduser()\n",
        "theme_dir.expanduser().mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "theme_manager = ThemeManager(theme_dir=theme_dir, themes=THEMES)\n",
        "theme_manager.list_themes()\n",
        "\n",
        "dark = theme_manager.get(\"dark\")\n",
        "light = theme_manager.get(\"light\") # Assign the light theme to the variable 'light'\n",
        "# theme_manager.preview_theme(dark)\n",
        "\n",
        "console = Console(theme=dark)"
      ],
      "metadata": {
        "id": "gOfQNc4AD5Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\", google_api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "P4aaLJEVIRh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "# from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "import glob # Import the glob module\n",
        "import fitz # Import fitz for read_pdf_to_string"
      ],
      "metadata": {
        "id": "L_K4F8E3ciqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HqrerQ9MnI1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunker = SemanticChunker(model, breakpoint_threshold_type=\"percentile\")"
      ],
      "metadata": {
        "id": "78rLqqXYk-nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf_to_string(path):\n",
        "    \"\"\"\n",
        "    Read a PDF document from the specified path and return its content as a string.\n",
        "\n",
        "    Args:\n",
        "        path (str): The file path to the PDF document.\n",
        "\n",
        "    Returns:\n",
        "        str: The concatenated text content of all pages in the PDF document.\n",
        "\n",
        "    The function uses the 'fitz' library (PyMuPDF) to open the PDF document, iterate over each page,\n",
        "    extract the text content from each page, and append it to a single string.\n",
        "    \"\"\"\n",
        "    # Open the PDF document located at the specified path\n",
        "    doc = fitz.open(path)\n",
        "    content = \"\"\n",
        "    # Iterate over each page in the document\n",
        "    for page_num in range(len(doc)):\n",
        "        # Get the current page\n",
        "        page = doc[page_num]\n",
        "        # Extract the text content from the current page and append it to the content string\n",
        "        content += page.get_text()\n",
        "    return content\n",
        "\n",
        "def replace_t_with_space(list_of_documents):\n",
        "    \"\"\"\n",
        "    Replaces all tab characters ('\\t') with spaces in the page content of each document\n",
        "\n",
        "    Args:\n",
        "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
        "\n",
        "    Returns:\n",
        "        The modified list of documents with tab characters replaced by spaces.\n",
        "    \"\"\"\n",
        "\n",
        "    for doc in list_of_documents:\n",
        "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
        "    return list_of_documents\n",
        "\n",
        "def encode_pdf(directory_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes all PDF files in a directory into a vector store using Hugging Face (sentence transformer) embeddings.\n",
        "\n",
        "    Args:\n",
        "        directory_path: The path to the directory containing PDF files.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    all_documents = [] # List to hold documents from all PDFs\n",
        "    pdf_files = glob.glob(f\"{directory_path}/*.pdf\") # Find all PDF files in the directory\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        loader = PyPDFLoader(pdf_file)\n",
        "        documents = loader.load()\n",
        "        all_documents.extend(documents) # Add documents from the current PDF to the list\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(all_documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "def encode_pdf_semantic(directory_path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes all PDF files in a directory into a vector store using Semantic Chunking and Hugging Face embeddings.\n",
        "\n",
        "    Args:\n",
        "        directory_path: The path to the directory containing PDF files.\n",
        "        chunk_size: The desired size of each text chunk (Note: SemanticChunker uses different logic).\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks (Note: SemanticChunker uses different logic).\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    all_documents = [] # List to hold documents from all PDFs\n",
        "    pdf_files = glob.glob(f\"{directory_path}/*.pdf\") # Find all PDF files in the directory\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        loader = PyPDFLoader(pdf_file)\n",
        "        documents = loader.load()\n",
        "        all_documents.extend(documents) # Add documents from the current PDF to the list\n",
        "\n",
        "    # Split documents into chunks using SemanticChunker\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    text_splitter = SemanticChunker(embeddings, breakpoint_threshold_type='percentile', breakpoint_threshold_amount=90)\n",
        "\n",
        "    texts = text_splitter.split_documents(all_documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "def retrieve_context_per_question(question, chunks_query_retriever):\n",
        "    \"\"\"\n",
        "    Retrieves relevant context and unique URLs for a given question using the chunks query retriever.\n",
        "\n",
        "    Args:\n",
        "        question: The question for which to retrieve context and URLs.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "        - A string with the concatenated content of relevant documents.\n",
        "        - A list of unique URLs from the metadata of the relevant documents.\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve relevant documents for the given question\n",
        "    docs = chunks_query_retriever.invoke(question)\n",
        "\n",
        "    # Concatenate document content\n",
        "    # context = \" \".join(doc.page_content for doc in docs)\n",
        "    context = [doc.page_content for doc in docs]\n",
        "\n",
        "    return context\n",
        "\n",
        "def show_context(context):\n",
        "  \"\"\"\n",
        "  Display the contents of the provided context list.\n",
        "\n",
        "  Args:\n",
        "      context (list): A list of context items to be displayed.\n",
        "\n",
        "  Prints each context item in the list with a heading indicating its position.\n",
        "  \"\"\"\n",
        "  for i, c in enumerate(context):\n",
        "      print(f\"Context {i + 1}:\")\n",
        "      print(c)\n",
        "      print(\"\\n\")"
      ],
      "metadata": {
        "id": "Y2BBhgzELoj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Data/uscis_aao_pdfs\"\n",
        "# load the pdf document\n",
        "chunks_vector_store = encode_pdf_semantic(path)"
      ],
      "metadata": {
        "id": "oLkUkw1j87vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_vector_store."
      ],
      "metadata": {
        "id": "d5nd5HkKmtXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "7FC-ger7AinX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "test_query = \"how many petitioner are total?\"\n",
        "context = retrieve_context_per_question(test_query, chunks_query_retriever)\n",
        "# show_context(context)\n",
        "\n",
        "# Set up system prompt\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Keep the answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{input}\"),\n",
        "\n",
        "])\n",
        "\n",
        "result = llm.invoke(prompt.format_prompt(context=context, input=test_query).to_messages())\n",
        "console.print(result)"
      ],
      "metadata": {
        "id": "-R5J7lW4104V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}